{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855148d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d685fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f07112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus - A large collection of text\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbd57c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eff5867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "print(len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5033c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = brown.sents(categories = 'adventure')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003f1115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4637"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac550cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dan Morgan told himself he would forget Ann Turner .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014f2a9",
   "metadata": {},
   "source": [
    "# Bag of Words Pipeline\n",
    "- Get the Data/Corpus\n",
    "- Tokenisation, Stopword Removal\n",
    "- Stemming\n",
    "- Building a Vocab\n",
    "- Vectorization\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee01fd",
   "metadata": {},
   "source": [
    "### Tokenisation, Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e0149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers.\n",
    "I went to the market to buy some fruits.\"\"\"\n",
    "\n",
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at prateek@cb.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1904044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de08bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.', 'The weather was cool and there were light showers.', 'I went to the market to buy some fruits.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(document)\n",
    "print(sents)\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08cac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcaba8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'prateek', '@', 'cb.com']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb291ae2",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a2f1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c881dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'should', 'shan', \"weren't\", 'no', 'mightn', 'this', 'over', 'how', 'won', 'any', 'before', 'had', \"won't\", 'against', 'into', 'off', 'after', 'both', 'has', 'them', 'your', 'but', 'with', \"mustn't\", 'not', \"doesn't\", 'nor', 'do', 'ours', 'which', 'll', \"you're\", 'himself', 're', \"needn't\", \"wouldn't\", 'herself', 'a', 'will', \"you'll\", 'during', 'myself', 'below', 'on', 'for', 'some', 'theirs', 'y', 'at', 'ma', 'we', 'in', 'out', 'these', 'it', 'themselves', 'further', 'd', 's', 'very', 'above', 'that', 'are', 'ourselves', 'm', 'he', 'to', 'having', 'between', \"she's\", 'hers', 'have', 't', 'same', \"couldn't\", 'again', 'and', 'their', \"don't\", 'where', 'me', 'than', 'isn', 'needn', 'down', \"didn't\", 'him', 'what', \"hasn't\", 'from', 'other', 'you', 'doesn', 'up', 'ain', 'yourselves', 'weren', 'most', 'why', \"aren't\", 'about', \"shouldn't\", \"you've\", 'they', 'own', 'when', 'those', 'her', 'while', 've', 'because', 'under', 'now', 'don', 'such', 'wouldn', 'few', 'too', 'was', \"mightn't\", 'wasn', 'being', 'or', 'she', 'through', 'hasn', 'been', 'didn', 'is', \"wasn't\", \"isn't\", \"it's\", 'i', 'our', 'yours', \"that'll\", 'yourself', 'then', 'of', 'an', 'just', 'who', 'here', \"should've\", 'couldn', 'doing', 'if', 'by', 'whom', 'were', 'his', 'once', 'more', 'the', 'aren', 'o', 'am', 'hadn', \"shan't\", 'so', 'shouldn', \"haven't\", 'all', 'itself', 'my', 'each', 'be', 'can', 'does', 'its', 'haven', 'there', \"hadn't\", 'did', 'only', 'mustn', 'as', 'until', \"you'd\"}\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d4be1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,stopwords):\n",
    "    useful_words = [w for w in text if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a19ec8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bothered', 'much']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am not bothered about her very much\".split()\n",
    "useful_text = remove_stopwords(text,sw)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "322a82e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'I' in sw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31174c38",
   "metadata": {},
   "source": [
    "### Tokenization using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae5ab471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abfd0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_text_ = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc1565e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " 'at',\n",
       " 'prateek@cb.com']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e1968",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "- Process that transforms particular words (verbs, plurals) into their radical form\n",
    "- Preserve the semantics of the sentence without increasing the number of unique tokens\n",
    "- Example : jumps, jumping, jumped, jump ==> jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e23bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"Foxes love to make jumps.The quick brown fox was seen jumping over the\n",
    "        lovely dog from a 6ft high wall\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24aaba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c569b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "967b5d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "036c450a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56f02799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer\n",
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b12f8d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e79fc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19f8dcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumping'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "wn.lemmatize('jumping')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bda27f",
   "metadata": {},
   "source": [
    "### Building a Vocab & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14530838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Corpus - Contains 4 documents, each document can have 1 or more sentence\n",
    "corpus = [\n",
    "    'Indian cricket team will win World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "    'We will win the next Lok Sabha elections, says confident Indian PM.',\n",
    "    'The nobel laurate won the hearts of the people.',\n",
    "    'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ca136b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74d40e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1947c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "241035eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0c86bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed1ce580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'win': 38, 'world': 40, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'the': 32, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'nobel': 20, 'laurate': 16, 'won': 39, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69945055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))\n",
    "print(len(cv.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "909c6df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse Mapping\n",
    "numbers = vectorized_corpus[2]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83cfca17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['hearts', 'laurate', 'nobel', 'of', 'people', 'the', 'won'],\n",
       "       dtype='<U9')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(numbers.reshape((1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548dce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    # Remove Stopwords\n",
    "    words = remove_stopwords(words,sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myTokenizer(sentence)\n",
    "#print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8add0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vectorized_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cb8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fabdde7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03fbe8f5",
   "metadata": {},
   "source": [
    "### More Ways to Create Features\n",
    "- Unigram - every word as a feature\n",
    "- Bigrams\n",
    "- Trigrams\n",
    "- n-grams\n",
    "- TF-IDF Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58c91641",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = [\"this is a good movie\"]\n",
    "sent_2 = [\"this is not a good movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a5f8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76f002d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [sent_1[0],sent_2[0]]\n",
    "cv_.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f447622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 4, 'is good': 1, 'good movie': 0, 'is not': 2, 'not good': 3}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e16cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7f43469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [sent_1[0],sent_2[0]]\n",
    "cv1.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ee75231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 2,\n",
       " 'good': 0,\n",
       " 'movie': 7,\n",
       " 'this is': 12,\n",
       " 'is good': 3,\n",
       " 'good movie': 1,\n",
       " 'this is good': 13,\n",
       " 'is good movie': 4,\n",
       " 'not': 8,\n",
       " 'is not': 5,\n",
       " 'not good': 9,\n",
       " 'this is not': 14,\n",
       " 'is not good': 6,\n",
       " 'not good movie': 10}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba1b848",
   "metadata": {},
   "source": [
    "### Tf-idf Normalization\n",
    "- Avoid features that occur very often, because they contain less information\n",
    "- Information decreases as the no of occurrences increase across different types of documents\n",
    "- So we define another term - term-document-frequency which associates a weight with every term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4afc8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1  = \"this is good movie\"\n",
    "sent_2 = \"this was good movie\"\n",
    "sent_3 = \"this is not good movie\"\n",
    "\n",
    "corpus = [sent_1,sent_2,sent_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c94a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "204665e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5602a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7b1afb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]\n",
      " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aeb1ca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ea4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
